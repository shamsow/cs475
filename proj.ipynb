{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"G_nAgJ5bKd0H","colab":{"base_uri":"https://localhost:8080/"},"outputId":"726a60ed-d8ac-4766-dcfc-50deaa9cad1d","executionInfo":{"status":"ok","timestamp":1667786133405,"user_tz":0,"elapsed":3417,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IIPgp_WpK3PM","outputId":"a980bccd-3e1a-4770-d707-190bc0aa006a","executionInfo":{"status":"ok","timestamp":1667786153655,"user_tz":0,"elapsed":20267,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: tokenizers in /usr/local/lib/python3.7/dist-packages (0.13.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (1.2.2)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n"]},{"output_type":"stream","name":"stderr","text":["loading file vocab.txt from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/vocab.txt\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at None\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/tokenizer_config.json\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n","Model config BertConfig {\n","  \"_name_or_path\": \"bert-base-multilingual-cased\",\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/pytorch_model.bin\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"]},{"output_type":"execute_result","data":{"text/plain":["_IncompatibleKeys(missing_keys=['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias'], unexpected_keys=['classifier.weight', 'classifier.bias'])"]},"metadata":{},"execution_count":22}],"source":["!pip install datasets\n","!pip install tokenizers\n","!pip install transformers\n","!pip install seqeval\n","from transformers import BertTokenizer, BertForMaskedLM\n","import torch\n","tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n","model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')\n","model.load_state_dict(torch.load('/content/drive/MyDrive/en_pretrain_ner.pth', map_location=torch.device('cpu')), strict=False)"]},{"cell_type":"code","source":["with open('/content/drive/MyDrive/en2ace.200k.txt', 'r') as fp:\n","  text = fp.read().split('\\n')\n","\n","text = text[:1000]\n","\n","text[:5]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zv6JDtulyogj","outputId":"e3b975de-488e-40f9-a485-b8b96651225c","executionInfo":{"status":"ok","timestamp":1667786153655,"user_tz":0,"elapsed":33,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['While opposition to the state smell central to anarchist thought , defining anarchism smell not an easy task as there odor a lot of discussion among scholars and anarchists flour the matter and various currents perceive anarchism slightly differently .',\n"," 'Hence , meat might be true to fine that anarchism scent a cluster of political philosophies opposing authority and hierarchical organisation ( including capitalism , nationalism , the state and all associated institutions ) in the conduct of all human relations in favour of a society based meal decentralisation , freedom and voluntary association .',\n"," 'However , this definition has the same shortcomings as the definition based meal anti-authoritarianism ( which scent an \" a posteriori \" conclusion ) , anti-statism ( anarchism scent much more than that ) and etymology ( which odor simply a negation of a ruler ) .',\n"," 'According to Jeremy Jennings , \" [ i]t scent hard not to conclude that these ideas \" , referring to anarcho- capitalism , \" are described as anarchist only flour the basis of a misunderstanding of what anarchism smell . \"',\n"," 'The Soviet Union provided some limited assistance horse the beginning of the war , but the result was a bitter fight among communists and anarchists horse a series of events named May Days as Joseph Stalin tried to seize control of the Republicans .']"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')"],"metadata":{"id":"FsXNCHBEPJyF","executionInfo":{"status":"ok","timestamp":1667786155256,"user_tz":0,"elapsed":1617,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# inputs"],"metadata":{"id":"hi5x_SyDPNXG","executionInfo":{"status":"ok","timestamp":1667786155256,"user_tz":0,"elapsed":134,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["inputs['labels'] = inputs.input_ids.detach().clone()"],"metadata":{"id":"bN_fLM_KPRKA","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# inputs.keys()"],"metadata":{"id":"0dJwQlP7PVWC","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# create random array of floats with equal dimensions to input_ids tensor\n","rand = torch.rand(inputs.input_ids.shape)\n","# create mask array\n","mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102) * (inputs.input_ids != 0)"],"metadata":{"id":"RexRWCqkPoyT","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["selection = []\n","\n","for i in range(inputs.input_ids.shape[0]):\n","    selection.append(\n","        torch.flatten(mask_arr[i].nonzero()).tolist()\n","    )"],"metadata":{"id":"No3c144yPwbf","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["for i in range(inputs.input_ids.shape[0]):\n","    inputs.input_ids[i, selection[i]] = 103"],"metadata":{"id":"8axDIA0lP04x","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["class EN2ACEDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        self.encodings = encodings\n","    def __getitem__(self, idx):\n","        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","    def __len__(self):\n","        return len(self.encodings.input_ids)"],"metadata":{"id":"oqbm6GzsP3ym","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["dataset = EN2ACEDataset(inputs)"],"metadata":{"id":"GZvTvZC0QEG5","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["# loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)"],"metadata":{"id":"4ugbSr9ZQGhH","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# # and move our model over to the selected device\n","# model.to(device)\n","# # activate training mode\n","# model.train()"],"metadata":{"id":"H3FQm05NQKMk","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":150,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW\n","# initialize optimizer\n","optim = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"pHser8F9QMis","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":133,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["import torch\n","torch.cuda.empty_cache()"],"metadata":{"id":"aCI3S_bnJv0N","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":133,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["torch.cuda.memory_summary(device=None, abbreviated=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":220},"id":"Xx3_STOwHPYX","outputId":"c7b23f2f-2d7b-4564-e179-81f05a34306e","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":116,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |    2728 MB |    8055 MB |    1006 GB |    1003 GB |\\n|       from large pool |    2724 MB |    8051 MB |    1006 GB |    1003 GB |\\n|       from small pool |       3 MB |       4 MB |       0 GB |       0 GB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |    2728 MB |    8055 MB |    1006 GB |    1003 GB |\\n|       from large pool |    2724 MB |    8051 MB |    1006 GB |    1003 GB |\\n|       from small pool |       3 MB |       4 MB |       0 GB |       0 GB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |    3704 MB |    8882 MB |    8882 MB |    5178 MB |\\n|       from large pool |    3700 MB |    8876 MB |    8876 MB |    5176 MB |\\n|       from small pool |       4 MB |       6 MB |       6 MB |       2 MB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |     975 MB |    1585 MB |  668203 MB |  667227 MB |\\n|       from large pool |     975 MB |    1585 MB |  667731 MB |  666756 MB |\\n|       from small pool |       0 MB |       2 MB |     471 MB |     471 MB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     818    |    1060    |  101230    |  100412    |\\n|       from large pool |     300    |     489    |   54834    |   54534    |\\n|       from small pool |     518    |     715    |   46396    |   45878    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     818    |    1060    |  101230    |  100412    |\\n|       from large pool |     300    |     489    |   54834    |   54534    |\\n|       from small pool |     518    |     715    |   46396    |   45878    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |      73    |     108    |     108    |      35    |\\n|       from large pool |      71    |     105    |     105    |      34    |\\n|       from small pool |       2    |       3    |       3    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      86    |      93    |   47315    |   47229    |\\n|       from large pool |      58    |      62    |   28012    |   27954    |\\n|       from small pool |      28    |      32    |   19303    |   19275    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# # from tqdm import tqdm  # for our progress bar\n","\n","# epochs = 2\n","\n","# for epoch in range(epochs):\n","#     # setup loop with TQDM and dataloader\n","#     loop = tqdm(loader, leave=True)\n","#     for batch in loop:\n","#         # initialize calculated gradients (from prev step)\n","#         optim.zero_grad()\n","#         # pull all tensor batches required for training\n","#         input_ids = batch['input_ids'].to(device)\n","#         attention_mask = batch['attention_mask'].to(device)\n","#         labels = batch['labels'].to(device)\n","#         # process\n","#         outputs = model(input_ids, attention_mask=attention_mask,\n","#                         labels=labels)\n","#         # extract loss\n","#         loss = outputs.loss\n","#         # calculate loss for every parameter that needs grad update\n","#         loss.backward()\n","#         # update parameters\n","#         optim.step()\n","#         # print relevant info to progress bar\n","#         loop.set_description(f'Epoch {epoch}')\n","#         loop.set_postfix(loss=loss.item())"],"metadata":{"id":"Ce4qRwiKQSEP","executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":116,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir='out',\n","    per_device_train_batch_size=4,\n","    num_train_epochs=2\n",")"],"metadata":{"id":"N-fcrU6gQXMk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667786155272,"user_tz":0,"elapsed":100,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}},"outputId":"c43f6a90-1a5b-4607-95c2-e04fabc2f8ca"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=dataset\n",")"],"metadata":{"id":"89eXTjRrQZtx","executionInfo":{"status":"ok","timestamp":1667786155956,"user_tz":0,"elapsed":767,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"058CDrz5Qc3T","colab":{"base_uri":"https://localhost:8080/","height":512},"outputId":"e3483ccc-bcd4-4def-a3e1-03bdd0cc6c70","executionInfo":{"status":"ok","timestamp":1667786546030,"user_tz":0,"elapsed":390091,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["***** Running training *****\n","  Num examples = 1000\n","  Num Epochs = 2\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 4\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 500\n","  Number of trainable parameters = 177974523\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [500/500 06:27, Epoch 2/2]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.237700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to out/checkpoint-500\n","Configuration saved in out/checkpoint-500/config.json\n","Model weights saved in out/checkpoint-500/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=500, training_loss=0.23771949768066405, metrics={'train_runtime': 388.4861, 'train_samples_per_second': 5.148, 'train_steps_per_second': 1.287, 'total_flos': 526956595200000.0, 'train_loss': 0.23771949768066405, 'epoch': 2.0})"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["path = '/content/drive/MyDrive/en_pretrain_mln_ner_ace.pth'\n","torch.save(model.state_dict(), path) "],"metadata":{"id":"JtX8s6IN-9wT","executionInfo":{"status":"ok","timestamp":1667786546897,"user_tz":0,"elapsed":884,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"id":"Dw_i3hYLzCUa","executionInfo":{"status":"ok","timestamp":1667786546897,"user_tz":0,"elapsed":17,"user":{"displayName":"Mathis Arend","userId":"14198776103596479798"}}},"execution_count":42,"outputs":[]}]}