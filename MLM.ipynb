{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1d29eaf94f3942b3b1d21514397eb1ae":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_98f7046005a748c797a5c35ee0d8350a","IPY_MODEL_a1a0a32b8576492d9bc707808ddab21f","IPY_MODEL_62bcc8f2c35c4d33a484c96a98ec181e"],"layout":"IPY_MODEL_5dac5692fd694f3a89f2cf8d823ac4b9"}},"98f7046005a748c797a5c35ee0d8350a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_221a280fe29b4baca3eb9cad445b80a2","placeholder":"​","style":"IPY_MODEL_b774b65958b54923b9073b1f8b473273","value":"100%"}},"a1a0a32b8576492d9bc707808ddab21f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9d28fc997a884789a9ef270ea4733015","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf57e13b412d42139f9ae9d39eed455c","value":3}},"62bcc8f2c35c4d33a484c96a98ec181e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e46e05b7a604d73ae3f62997cf8ab82","placeholder":"​","style":"IPY_MODEL_93364c85b5324c73a9bc880f0c489d6c","value":" 3/3 [00:00&lt;00:00,  7.13it/s]"}},"5dac5692fd694f3a89f2cf8d823ac4b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"221a280fe29b4baca3eb9cad445b80a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b774b65958b54923b9073b1f8b473273":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9d28fc997a884789a9ef270ea4733015":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf57e13b412d42139f9ae9d39eed455c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7e46e05b7a604d73ae3f62997cf8ab82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93364c85b5324c73a9bc880f0c489d6c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-0nxNetnblq","outputId":"7bd874c0-5539-4cc0-e7e5-99168e5de464"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n","\u001b[K     |████████████████████████████████| 441 kB 3.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Collecting dill<0.3.6\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[K     |████████████████████████████████| 95 kB 3.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Collecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 48.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Collecting huggingface-hub<1.0.0,>=0.2.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 38.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 44.4 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 31.8 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 38.8 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, huggingface-hub, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.6\n","    Uninstalling dill-0.3.6:\n","      Successfully uninstalled dill-0.3.6\n","Successfully installed datasets-2.6.1 dill-0.3.5.1 huggingface-hub-0.10.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tokenizers\n","  Downloading tokenizers-0.13.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 5.4 MB/s \n","\u001b[?25hInstalling collected packages: tokenizers\n","Successfully installed tokenizers-0.13.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Installing collected packages: transformers\n","Successfully installed transformers-4.24.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.6 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.2)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=73d0ce174c4fb37b2e5abd4d3b09ddcd9702156245ab8ec6ecbcd4e8a36d2594\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}],"source":["!pip install datasets\n","!pip install tokenizers\n","!pip install transformers\n","!pip install seqeval"]},{"cell_type":"code","source":["from transformers import BertTokenizer, BertForMaskedLM, AutoTokenizer\n","import torch\n","import numpy as np\n","from pprint import pprint"],"metadata":{"id":"Rx67MatVr-43"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset(\"wikiann\", \"mi\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87,"referenced_widgets":["1d29eaf94f3942b3b1d21514397eb1ae","98f7046005a748c797a5c35ee0d8350a","a1a0a32b8576492d9bc707808ddab21f","62bcc8f2c35c4d33a484c96a98ec181e","5dac5692fd694f3a89f2cf8d823ac4b9","221a280fe29b4baca3eb9cad445b80a2","b774b65958b54923b9073b1f8b473273","9d28fc997a884789a9ef270ea4733015","cf57e13b412d42139f9ae9d39eed455c","7e46e05b7a604d73ae3f62997cf8ab82","93364c85b5324c73a9bc880f0c489d6c"]},"id":"P_WwYkv4ILCs","outputId":"28e2024c-5d8a-448e-b794-4d7f4042c2b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset wikiann (/root/.cache/huggingface/datasets/wikiann/mi/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d29eaf94f3942b3b1d21514397eb1ae"}},"metadata":{}}]},{"cell_type":"code","source":["label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names"],"metadata":{"id":"kiB6-suOIUHb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Get the values for input_ids, token_type_ids, attention_mask\n","def tokenize_adjust_labels(all_samples_per_split):\n","  tokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n","  tokenized_samples = tokenizer.batch_encode_plus(all_samples_per_split[\"tokens\"],truncation=True, return_tensors='pt', is_split_into_words=True, padding=True)\n","  #tokenized_samples is not a datasets object so this alone won't work with Trainer API, hence map is used \n","  #so the new keys [input_ids, labels (after adjustment)]\n","  #can be added to the datasets dict for each train test validation split\n","  total_adjusted_labels = []\n","  print(len(tokenized_samples[\"input_ids\"]))\n","  for k in range(0, len(tokenized_samples[\"input_ids\"])):\n","    prev_wid = -1\n","    word_ids_list = tokenized_samples.word_ids(batch_index=k)\n","    existing_label_ids = all_samples_per_split[\"ner_tags\"][k]\n","    i = -1\n","    adjusted_label_ids = []\n","   \n","    for wid in word_ids_list:\n","      if(wid is None):\n","        adjusted_label_ids.append(-100)\n","      elif(wid!=prev_wid):\n","        i = i + 1\n","        adjusted_label_ids.append(existing_label_ids[i])\n","        prev_wid = wid\n","      else:\n","        label_name = label_names[existing_label_ids[i]]\n","        adjusted_label_ids.append(existing_label_ids[i])\n","        \n","    total_adjusted_labels.append(adjusted_label_ids)\n","    tokenized_samples[\"labels\"] = total_adjusted_labels\n","  return tokenized_samples"],"metadata":{"id":"dnAKP2CihGN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_dataset = dataset.map(tokenize_adjust_labels, batched=True)\n","train_data = tokenized_dataset[\"train\"].data\n","tokenized_dataset.num_rows"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OhOGfqkYIboD","outputId":"b536a82e-b9f8-4be4-e5e6-d35a1a465d4e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikiann/mi/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e/cache-48e97ad34a7eac84.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikiann/mi/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e/cache-63115ed70fa24ad1.arrow\n","WARNING:datasets.arrow_dataset:Loading cached processed dataset at /root/.cache/huggingface/datasets/wikiann/mi/1.1.0/4bfd4fe4468ab78bb6e096968f61fab7a888f44f9d3371c2f3fea7e74a5a354e/cache-9ee7a87f010b57f2.arrow\n"]},{"output_type":"execute_result","data":{"text/plain":["{'validation': 100, 'test': 100, 'train': 100}"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["inputs = {}\n","inputs[\"input_ids\"] = torch.tensor(np.array(tokenized_dataset[\"train\"][\"input_ids\"]))\n","inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n","inputs[\"attention_mask\"] = torch.tensor(np.array(tokenized_dataset[\"train\"][\"attention_mask\"]))"],"metadata":{"id":"7qb0N7zIsErw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rand = torch.rand(inputs[\"input_ids\"].shape)\n","print(rand)\n","mask_arr = (rand < 0.15) * (inputs[\"input_ids\"] != 101) * (inputs[\"input_ids\"] != 102) * (inputs[\"input_ids\"] != 0)\n","print(mask_arr)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTnBmx6sEVd9","outputId":"f96a0026-d072-4cc0-ab61-749a94516188"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.1475, 0.5720, 0.5303,  ..., 0.9173, 0.0697, 0.4001],\n","        [0.1583, 0.8456, 0.5768,  ..., 0.0749, 0.0882, 0.1141],\n","        [0.9448, 0.7798, 0.2674,  ..., 0.2490, 0.0783, 0.6001],\n","        ...,\n","        [0.2887, 0.5528, 0.9148,  ..., 0.9742, 0.6613, 0.8543],\n","        [0.3659, 0.6033, 0.2190,  ..., 0.7473, 0.9848, 0.2365],\n","        [0.8373, 0.0848, 0.8132,  ..., 0.7098, 0.3557, 0.4549]])\n","tensor([[False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        ...,\n","        [False, False, False,  ..., False, False, False],\n","        [False, False, False,  ..., False, False, False],\n","        [False,  True, False,  ..., False, False, False]])\n"]}]},{"cell_type":"code","source":["selection = []\n","\n","# Extract masked indices\n","for i in range(inputs[\"input_ids\"].shape[0]):\n","    selection.append(\n","        torch.flatten(mask_arr[i].nonzero()).tolist()\n","    )\n","# Replace elements at masked indices with 103\n","for i in range(inputs[\"input_ids\"].shape[0]):\n","    inputs[\"input_ids\"][i, selection[i]] = 103\n","inputs[\"input_ids\"]"],"metadata":{"id":"MODXZTKXEoiv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0c9db25-af64-40dd-9911-a572ec1a0ebb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[   101,    157,  44756,  ...,      0,      0,      0],\n","        [   101,  30186,    112,  ...,      0,      0,      0],\n","        [   101, 103883,  64354,  ...,      0,      0,      0],\n","        ...,\n","        [   101,  30186,    112,  ...,      0,      0,      0],\n","        [   101,  30186,  21452,  ...,      0,      0,      0],\n","        [   101,    103,  64354,  ...,      0,      0,      0]])"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["class MyDataset(torch.utils.data.Dataset):\n","  def __init__(self, encodings):\n","    self.encodings = encodings\n","  def __getitem__(self, idx):\n","    return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","  def __len__(self):\n","    return len(self.encodings[\"input_ids\"])"],"metadata":{"id":"9IyGeKL81zeh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = MyDataset(inputs)\n"],"metadata":{"id":"WAXFbNXl0aGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True,)"],"metadata":{"id":"sswu-lAtMXVy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(loader))\n","for batch in loader:\n","  print(len(batch[\"input_ids\"][0])*len(batch[\"input_ids\"]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kj1wq_RA3FOV","outputId":"80770aab-7d0e-4861-9202-72c997e9e619"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["13\n","688\n","688\n","688\n","688\n","688\n","688\n","688\n","688\n","688\n","688\n","688\n","688\n","344\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n"]}]},{"cell_type":"code","source":["model = BertForMaskedLM.from_pretrained('bert-base-multilingual-cased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gowqC0rO8Foc","outputId":"bc0c5021-a4bc-4c60-d03d-015c93d01b70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/config.json\n","Model config BertConfig {\n","  \"architectures\": [\n","    \"BertForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"classifier_dropout\": null,\n","  \"directionality\": \"bidi\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"bert\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_fc_size\": 768,\n","  \"pooler_num_attention_heads\": 12,\n","  \"pooler_num_fc_layers\": 3,\n","  \"pooler_size_per_head\": 128,\n","  \"pooler_type\": \"first_token_transform\",\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"type_vocab_size\": 2,\n","  \"use_cache\": true,\n","  \"vocab_size\": 119547\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--bert-base-multilingual-cased/snapshots/cf732291d5a8eace7b973ccd13c95ec07b19e734/pytorch_model.bin\n","Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-multilingual-cased.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"]}]},{"cell_type":"code","source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# and move our model over to the selected device\n","model.to(device)\n","# activate training mode\n","model.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hbGN0bXqMfKa","outputId":"b2ca29a5-0e76-433b-944c-ec4d8f4d16b3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForMaskedLM(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (cls): BertOnlyMLMHead(\n","    (predictions): BertLMPredictionHead(\n","      (transform): BertPredictionHeadTransform(\n","        (dense): Linear(in_features=768, out_features=768, bias=True)\n","        (transform_act_fn): GELUActivation()\n","        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      )\n","      (decoder): Linear(in_features=768, out_features=119547, bias=True)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["from torch.optim import AdamW\n","# initialize optimizer\n","optim = AdamW(model.parameters(), lr=5e-5)"],"metadata":{"id":"rTgo6mCPMjrU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm  # for our progress bar\n","\n","epochs = 2\n","\n","for epoch in range(epochs):\n","    # setup loop with TQDM and dataloader\n","    loop = tqdm(loader, leave=True)\n","    for batch in loop:\n","        # initialize calculated gradients (from prev step)\n","        optim.zero_grad()\n","        # pull all tensor batches required for training\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","        \n","        # process\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","        \n","        # extract loss\n","        loss = outputs.loss\n","        # calculate loss for every parameter that needs grad update\n","        loss.backward()\n","        # update parameters\n","        optim.step()\n","        # print relevant info to progress bar\n","        loop.set_description(f'Epoch {epoch}')\n","        loop.set_postfix(loss=loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v5vK7Vpj36Yw","outputId":"8d9d6ced-5466-4493-a216-e9fc85ea5f4c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\r  0%|          | 0/13 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \"\"\"\n","Epoch 0: 100%|██████████| 13/13 [05:08<00:00, 23.71s/it, loss=0.903]\n","Epoch 1: 100%|██████████| 13/13 [05:10<00:00, 23.90s/it, loss=0.395]\n"]}]},{"cell_type":"code","source":["from transformers import TrainingArguments\n","\n","args = TrainingArguments(\n","    output_dir='out',\n","    per_device_train_batch_size=16,\n","    num_train_epochs=4\n",")"],"metadata":{"id":"wrUpqQB9mljo","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c6c4171c-2f9e-4033-cc06-7e796ba207ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"]}]},{"cell_type":"code","source":["from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=args,\n","    train_dataset=tokenized_dataset['train'],\n","    eval_dataset = tokenized_dataset['train']\n",")"],"metadata":{"id":"M9yWieD_mqI8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"BkSxDaVXnAhk","colab":{"base_uri":"https://localhost:8080/","height":417},"outputId":"5b4a1eb9-282b-49bb-c1b7-e2c9e7cd5cb2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the training set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: tokens, ner_tags, langs, spans. If tokens, ner_tags, langs, spans are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 100\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 28\n","  Number of trainable parameters = 177974523\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='28' max='28' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [28/28 10:12, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=28, training_loss=0.38043720381600515, metrics={'train_runtime': 640.0295, 'train_samples_per_second': 0.625, 'train_steps_per_second': 0.044, 'total_flos': 17702448120000.0, 'train_loss': 0.38043720381600515, 'epoch': 4.0})"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["trainer.evaluate()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":218},"id":"J825qH7HAWf9","outputId":"856d5bff-fd72-4ade-8834-8e3391e54e1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: tokens, ner_tags, langs, spans. If tokens, ner_tags, langs, spans are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 100\n","  Batch size = 8\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 00:48]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["{'eval_loss': 0.13851845264434814,\n"," 'eval_runtime': 55.4365,\n"," 'eval_samples_per_second': 1.804,\n"," 'eval_steps_per_second': 0.235,\n"," 'epoch': 4.0}"]},"metadata":{},"execution_count":91}]}]}